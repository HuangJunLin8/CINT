@article{zhao2023learning,
  title={Learning fine-grained bimanual manipulation with low-cost hardware},
  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2304.13705},
  year={2023}
}



@misc{fu_deep_2022,
	title = {Deep {Whole}-{Body} {Control}: {Learning} a {Unified} {Policy} for {Manipulation} and {Locomotion}},
	shorttitle = {Deep {Whole}-{Body} {Control}},
	url = {http://arxiv.org/abs/2210.10044},
	doi = {10.48550/arXiv.2210.10044},
	abstract = {An attached arm can significantly increase the applicability of legged robots to several mobile manipulation tasks that are not possible for the wheeled or tracked counterparts. The standard hierarchical control pipeline for such legged manipulators is to decouple the controller into that of manipulation and locomotion. However, this is ineffective. It requires immense engineering to support coordination between the arm and legs, and error can propagate across modules causing non-smooth unnatural motions. It is also biological implausible given evidence for strong motor synergies across limbs. In this work, we propose to learn a unified policy for whole-body control of a legged manipulator using reinforcement learning. We propose Regularized Online Adaptation to bridge the Sim2Real gap for high-DoF control, and Advantage Mixing exploiting the causal dependency in the action space to overcome local minima during training the whole-body system. We also present a simple design for a low-cost legged manipulator, and find that our unified policy can demonstrate dynamic and agile behaviors across several task setups. Videos are at https://maniploco.github.io},
	urldate = {2025-03-31},
	publisher = {arXiv},
	author = {Fu, Zipeng and Cheng, Xuxin and Pathak, Deepak},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10044 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: CoRL 2022 (Oral). Project website at https://maniploco.github.io},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\YZAMPLCT\\Fu 等 - 2022 - Deep Whole-Body Control Learning a Unified Policy for Manipulation and Locomotion.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\IZKDZXKT\\2210.html:text/html},
}


@inproceedings{ref4,
	title = {Towards a personal robotics development platform: Rationale and design of an intrinsically safe personal robot},
	url = {https://ieeexplore.ieee.org/document/4543527/?arnumber=4543527},
	doi = {10.1109/ROBOT.2008.4543527},
	shorttitle = {Towards a personal robotics development platform},
	abstract = {The most critical challenge for Personal Robotics is to manage the issue of human safety and yet provide the physical capability to perform useful work. This paper describes a novel concept for a mobile, 2-armed, 25-degree-offreedom system with backdrivable joints, low mechanical impedance, and a 5 kg payload per arm. System identification, design safety calculations and performance evaluation studies of the first prototype are included, as well as plans for a future development.},
	eventtitle = {2008 {IEEE} International Conference on Robotics and Automation},
	pages = {2165--2170},
	booktitle = {2008 {IEEE} International Conference on Robotics and Automation},
	author = {Wyrobek, Keenan A. and Berger, Eric H. and Van der Loos, H.F. Machiel and Salisbury, J. Kenneth},
	urldate = {2025-03-31},
	date = {2008-05},
	note = {{ISSN}: 1050-4729},
	keywords = {Force control, Human robot interaction, Humanoid robots, {ISO} standards, Orbital robotics, Payloads, Rehabilitation robotics, Robotics and automation, Service robots, Software safety},
	file = {Full Text PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\LENNW2BI\\Wyrobek 等 - 2008 - Towards a personal robotics development platform Rationale and design of an intrinsically safe pers.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\VHWD8D72\\4543527.html:text/html},
}

@misc{ref6,
	title = {Vision-Based Manipulators Need to Also See from Their Hands},
	url = {http://arxiv.org/abs/2203.12677},
	doi = {10.48550/arXiv.2203.12677},
	abstract = {We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation.},
	number = {{arXiv}:2203.12677},
	publisher = {{arXiv}},
	author = {Hsu, Kyle and Kim, Moo Jin and Rafailov, Rafael and Wu, Jiajun and Finn, Chelsea},
	urldate = {2025-03-31},
	date = {2022-03-15},
	eprinttype = {arxiv},
	eprint = {2203.12677 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\MTU2G25Z\\Hsu 等 - 2022 - Vision-Based Manipulators Need to Also See from Their Hands.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\T3AFEZKA\\2203.html:text/html},
}

@misc{zhao_learning_2023,
	title = {Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware},
	url = {http://arxiv.org/abs/2304.13705},
	doi = {10.48550/arXiv.2304.13705},
	abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers ({ACT}), which learns a generative model over action sequences. {ACT} allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
	number = {{arXiv}:2304.13705},
	publisher = {{arXiv}},
	author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
	urldate = {2025-03-31},
	date = {2023-04-23},
	eprinttype = {arxiv},
	eprint = {2304.13705 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\JGN6EPRF\\Zhao 等 - 2023 - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\TQLSLN9Z\\2304.html:text/html},
}

@misc{ref8,
	title = {Waypoint-Based Imitation Learning for Robotic Manipulation},
	url = {http://arxiv.org/abs/2307.14326},
	doi = {10.48550/arXiv.2307.14326},
	abstract = {While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning ({BC}). Waypoints can help address this problem by reducing the horizon of the learning problem for {BC}, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction ({AWE}) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. {AWE} can be combined with any {BC} algorithm, and we find that {AWE} can increase the success rate of state-of-the-art algorithms by up to 25\% in simulation and by 4-28\% on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/},
	number = {{arXiv}:2307.14326},
	publisher = {{arXiv}},
	author = {Shi, Lucy Xiaoyang and Sharma, Archit and Zhao, Tony Z. and Finn, Chelsea},
	urldate = {2025-03-31},
	date = {2023-07-26},
	eprinttype = {arxiv},
	eprint = {2307.14326 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\ZRDWS4KL\\Shi 等 - 2023 - Waypoint-Based Imitation Learning for Robotic Manipulation.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\QJGDFLBB\\2307.html:text/html},
}

@article{ref11,
	title = {Dual arm manipulation—A survey},
	volume = {60},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092188901200108X},
	doi = {10.1016/j.robot.2012.07.005},
	abstract = {Recent advances in both anthropomorphic robots and bimanual industrial manipulators had led to an increased interest in the speciﬁc problems pertaining to dual arm manipulation. For the future, we foresee robots performing humanlike tasks in both domestic and industrial settings. It is therefore natural to study speciﬁcs of dual arm manipulation in humans and methods for using the resulting knowledge in robot control. The related scientiﬁc problems range from low-level control to high level task planning and execution. This review aims to summarize the current state of the art from the heterogenous range of ﬁelds that study the diﬀerent aspects of these problems speciﬁcally in dual arm manipulation.},
	pages = {1340--1353},
	number = {10},
	journaltitle = {Robotics and Autonomous Systems},
	shortjournal = {Robotics and Autonomous Systems},
	author = {Smith, Christian and Karayiannidis, Yiannis and Nalpantidis, Lazaros and Gratal, Xavi and Qi, Peng and Dimarogonas, Dimos V. and Kragic, Danica},
	urldate = {2025-03-31},
	date = {2012-10},
	langid = {english},
	file = {PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\XAB2VANJ\\Smith 等 - 2012 - Dual arm manipulation—A survey.pdf:application/pdf},
}

@misc{ref12,
	title = {Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
	url = {http://arxiv.org/abs/2303.04137},
	doi = {10.48550/arXiv.2303.04137},
	shorttitle = {Diffusion Policy},
	abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
	number = {{arXiv}:2303.04137},
	publisher = {{arXiv}},
	author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
	urldate = {2025-03-31},
	date = {2024-03-14},
	eprinttype = {arxiv},
	eprint = {2303.04137 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\3INEVD6K\\Chi 等 - 2024 - Diffusion Policy Visuomotor Policy Learning via Action Diffusion.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\DKN3SNSB\\2303.html:text/html},
}

@inproceedings{ref16,
  title={Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0},
  author={O’Neill, Abby and Rehman, Abdul and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and Jain, Ajinkya and others},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={6892--6903},
  year={2024},
  organization={IEEE}
}

@inproceedings{ref7,
	location = {Seattle, {WA}, {USA}},
	title = {Towards learning hierarchical skills for multi-phase manipulation tasks},
	isbn = {978-1-4799-6923-4},
	url = {http://ieeexplore.ieee.org/document/7139389/},
	doi = {10.1109/ICRA.2015.7139389},
	abstract = {Most manipulation tasks can be decomposed into a sequence of phases, where the robot’s actions have different effects in each phase. The robot can perform actions to transition between phases and, thus, alter the effects of its actions, e.g. grasp an object in order to then lift it. The robot can thus reach a phase that affords the desired manipulation. In this paper, we present an approach for exploiting the phase structure of tasks in order to learn manipulation skills more efﬁciently. Starting with human demonstrations, the robot learns a probabilistic model of the phases and the phase transitions. The robot then employs model-based reinforcement learning to create a library of motor primitives for transitioning between phases. The learned motor primitives generalize to new situations and tasks. Given this library, the robot uses a value function approach to learn a high-level policy for sequencing the motor primitives. The proposed method was successfully evaluated on a real robot performing a bimanual grasping task.},
	eventtitle = {2015 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {1503--1510},
	booktitle = {2015 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Kroemer, Oliver and Daniel, Christian and Neumann, Gerhard and Van Hoof, Herke and Peters, Jan},
	urldate = {2025-03-31},
	date = {2015-05},
	langid = {english},
	file = {PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\HBI8S8T4\\Kroemer 等 - 2015 - Towards learning hierarchical skills for multi-phase manipulation tasks.pdf:application/pdf},
}

@misc{ref9,
	title = {Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias},
	url = {http://arxiv.org/abs/1807.07049},
	doi = {10.48550/arXiv.1807.07049},
	shorttitle = {Robot Learning in Homes},
	abstract = {Data-driven approaches to solving robotic tasks have gained a lot of traction in recent years. However, most existing policies are trained on large-scale datasets collected in curated lab settings. If we aim to deploy these models in unstructured visual environments like people's homes, they will be unable to cope with the mismatch in data distribution. In such light, we present the first systematic effort in collecting a large dataset for robotic grasping in homes. First, to scale and parallelize data collection, we built a low cost mobile manipulator assembled for under 3K {USD}. Second, data collected using low cost robots suffer from noisy labels due to imperfect execution and calibration errors. To handle this, we develop a framework which factors out the noise as a latent variable. Our model is trained on 28K grasps collected in several houses under an array of different environmental conditions. We evaluate our models by physically executing grasps on a collection of novel objects in multiple unseen homes. The models trained with our home dataset showed a marked improvement of 43.7\% over a baseline model trained with data collected in lab. Our architecture which explicitly models the latent noise in the dataset also performed 10\% better than one that did not factor out the noise. We hope this effort inspires the robotics community to look outside the lab and embrace learning based approaches to handle inaccurate cheap robots.},
	number = {{arXiv}:1807.07049},
	publisher = {{arXiv}},
	author = {Gupta, Abhinav and Murali, Adithyavairavan and Gandhi, Dhiraj and Pinto, Lerrel},
	urldate = {2025-03-31},
	date = {2018-07-18},
	eprinttype = {arxiv},
	eprint = {1807.07049 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\6T4ITUBG\\Gupta 等 - 2018 - Robot Learning in Homes Improving Generalization and Reducing Dataset Bias.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\6N2VGCB7\\1807.html:text/html},
}

@article{ref13,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@misc{ref14,
	title = {Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion},
	url = {http://arxiv.org/abs/2210.10044},
	doi = {10.48550/arXiv.2210.10044},
	shorttitle = {Deep Whole-Body Control},
	abstract = {An attached arm can significantly increase the applicability of legged robots to several mobile manipulation tasks that are not possible for the wheeled or tracked counterparts. The standard hierarchical control pipeline for such legged manipulators is to decouple the controller into that of manipulation and locomotion. However, this is ineffective. It requires immense engineering to support coordination between the arm and legs, and error can propagate across modules causing non-smooth unnatural motions. It is also biological implausible given evidence for strong motor synergies across limbs. In this work, we propose to learn a unified policy for whole-body control of a legged manipulator using reinforcement learning. We propose Regularized Online Adaptation to bridge the Sim2Real gap for high-{DoF} control, and Advantage Mixing exploiting the causal dependency in the action space to overcome local minima during training the whole-body system. We also present a simple design for a low-cost legged manipulator, and find that our unified policy can demonstrate dynamic and agile behaviors across several task setups. Videos are at https://maniploco.github.io},
	number = {{arXiv}:2210.10044},
	publisher = {{arXiv}},
	author = {Fu, Zipeng and Cheng, Xuxin and Pathak, Deepak},
	urldate = {2025-03-31},
	date = {2022-10-18},
	eprinttype = {arxiv},
	eprint = {2210.10044 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\YZAMPLCT\\Fu 等 - 2022 - Deep Whole-Body Control Learning a Unified Policy for Manipulation and Locomotion.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\IZKDZXKT\\2210.html:text/html},
}

@inproceedings{ref15,
	location = {Tokyo, Japan},
	title = {Legibility and predictability of robot motion},
	isbn = {978-1-4673-3101-2 978-1-4673-3099-2 978-1-4673-3100-5},
	url = {http://ieeexplore.ieee.org/document/6483603/},
	doi = {10.1109/HRI.2013.6483603},
	abstract = {A key requirement for seamless human-robot collaboration is for the robot to make its intentions clear to its human collaborator. A collaborative robot’s motion must be legible, or intent-expressive. Legibility is often described in the literature as and effect of predictable, unsurprising, or expected motion. Our central insight is that predictability and legibility are fundamentally different and often contradictory properties of motion. We develop a formalism to mathematically deﬁne and distinguish predictability and legibility of motion. We formalize the two based on inferences between trajectories and goals in opposing directions, drawing the analogy to action interpretation in psychology. We then propose mathematical models for these inferences based on optimizing cost, drawing the analogy to the principle of rational action. Our experiments validate our formalism’s prediction that predictability and legibility can contradict, and provide support for our models. Our ﬁndings indicate that for robots to seamlessly collaborate with humans, they must change the way they plan their motion.},
	eventtitle = {2013 8th {ACM}/{IEEE} International Conference on Human-Robot Interaction ({HRI})},
	pages = {301--308},
	booktitle = {2013 8th {ACM}/{IEEE} International Conference on Human-Robot Interaction ({HRI})},
	publisher = {{IEEE}},
	author = {Dragan, Anca D. and Lee, Kenton C.T. and Srinivasa, Siddhartha S.},
	urldate = {2025-03-31},
	date = {2013-03},
	langid = {english},
	file = {PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\FEWYXIPY\\Dragan 等 - 2013 - Legibility and predictability of robot motion.pdf:application/pdf},
}

@online{ref1,
	title = {{ViperX} 300 S},
	url = {https://www.trossenrobotics.com/viperx-300},
	abstract = {Our largest and most capable robotic research manipulator arm. 750g payload capacity and 6 Degrees of Freedom ({DoF}).},
	titleaddon = {Trossen Robotics},
	urldate = {2025-03-31},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\ALKFFCJ4\\viperx-300.html:text/html},
}

@online{ref2,
	title = {{WidowX} 250 S},
	url = {https://www.trossenrobotics.com/widowx-250},
	abstract = {The go-to robotic research manipulator arm for teleoperation tasks. 250g payload capacity and 6 Degrees of Freedom ({DoF}).},
	titleaddon = {Trossen Robotics},
	urldate = {2025-03-31},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\F97S5XFN\\widowx-250.html:text/html},
}

@video{ref3,
	title = {Highly Dexterous Manipulation System - Capabilities - Part 1},
	url = {https://www.youtube.com/watch?v=TearcKVj0iY},
	author = {{RE2, Inc.}},
	urldate = {2025-03-31},
	date = {2014-11-08},
}

@misc{ref10,
	title = {Behavior Transformers: Cloning \$k\$ modes with one stone},
	url = {http://arxiv.org/abs/2206.11251},
	doi = {10.48550/arXiv.2206.11251},
	shorttitle = {Behavior Transformers},
	abstract = {While behavior learning has made impressive progress in recent times, it lags behind computer vision and natural language processing due to its inability to leverage large, human-generated datasets. Human behaviors have wide variance, multiple modes, and human demonstrations typically do not come with reward labels. These properties limit the applicability of current methods in Offline {RL} and Behavioral Cloning to learn from large, pre-collected datasets. In this work, we present Behavior Transformer ({BeT}), a new technique to model unlabeled demonstration data with multiple modes. {BeT} retrofits standard transformer architectures with action discretization coupled with a multi-task action correction inspired by offset prediction in object detection. This allows us to leverage the multi-modal modeling ability of modern transformers to predict multi-modal continuous actions. We experimentally evaluate {BeT} on a variety of robotic manipulation and self-driving behavior datasets. We show that {BeT} significantly improves over prior state-of-the-art work on solving demonstrated tasks while capturing the major modes present in the pre-collected datasets. Finally, through an extensive ablation study, we analyze the importance of every crucial component in {BeT}. Videos of behavior generated by {BeT} are available at https://notmahi.github.io/bet},
	number = {{arXiv}:2206.11251},
	publisher = {{arXiv}},
	author = {Shafiullah, Nur Muhammad Mahi and Cui, Zichen Jeff and Altanzaya, Ariuntuya and Pinto, Lerrel},
	urldate = {2025-03-31},
	date = {2022-10-11},
	eprinttype = {arxiv},
	eprint = {2206.11251 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\IKR7BXNT\\Shafiullah 等 - 2022 - Behavior Transformers Cloning \$k\$ modes with one stone.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\GWJKVUH4\\2206.html:text/html},
}

@misc{ref5,
	title = {The Surprising Effectiveness of Representation Learning for Visual Imitation},
	url = {http://arxiv.org/abs/2112.01511},
	doi = {10.48550/arXiv.2112.01511},
	abstract = {While visual imitation learning offers one of the most effective ways of learning from visual demonstrations, generalizing from them requires either hundreds of diverse demonstrations, task specific priors, or large, hard-to-train parametric models. One reason such complexities arise is because standard visual imitation frameworks try to solve two coupled problems at once: learning a succinct but good representation from the diverse visual data, while simultaneously learning to associate the demonstrated actions with such representations. Such joint learning causes an interdependence between these two problems, which often results in needing large amounts of demonstrations for learning. To address this challenge, we instead propose to decouple representation learning from behavior learning for visual imitation. First, we learn a visual representation encoder from offline data using standard supervised and self-supervised learning methods. Once the representations are trained, we use non-parametric Locally Weighted Regression to predict the actions. We experimentally show that this simple decoupling improves the performance of visual imitation models on both offline demonstration datasets and real-robot door opening compared to prior work in visual imitation. All of our generated data, code, and robot videos are publicly available at https://jyopari.github.io/{VINN}/.},
	number = {{arXiv}:2112.01511},
	publisher = {{arXiv}},
	author = {Pari, Jyothish and Shafiullah, Nur Muhammad and Arunachalam, Sridhar Pandian and Pinto, Lerrel},
	urldate = {2025-03-31},
	date = {2021-12-06},
	eprinttype = {arxiv},
	eprint = {2112.01511 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\LHWENSCG\\Pari 等 - 2021 - The Surprising Effectiveness of Representation Learning for Visual Imitation.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\KFV2KFVV\\2112.html:text/html},
}

@misc{ref17,
	title = {On Bringing Robots Home},
	url = {http://arxiv.org/abs/2311.16098},
	doi = {10.48550/arXiv.2311.16098},
	abstract = {Throughout history, we have successfully integrated various machines into our homes. Dishwashers, laundry machines, stand mixers, and robot vacuums are a few recent examples. However, these machines excel at performing only a single task effectively. The concept of a "generalist machine" in homes - a domestic assistant that can adapt and learn from our needs, all while remaining cost-effective - has long been a goal in robotics that has been steadily pursued for decades. In this work, we initiate a large-scale effort towards this goal by introducing Dobb-E, an affordable yet versatile general-purpose system for learning robotic manipulation within household settings. Dobb-E can learn a new task with only five minutes of a user showing it how to do it, thanks to a demonstration collection tool ("The Stick") we built out of cheap parts and {iPhones}. We use the Stick to collect 13 hours of data in 22 homes of New York City, and train Home Pretrained Representations ({HPR}). Then, in a novel home environment, with five minutes of demonstrations and fifteen minutes of adapting the {HPR} model, we show that Dobb-E can reliably solve the task on the Stretch, a mobile robot readily available on the market. Across roughly 30 days of experimentation in homes of New York City and surrounding areas, we test our system in 10 homes, with a total of 109 tasks in different environments, and finally achieve a success rate of 81\%. Beyond success percentages, our experiments reveal a plethora of unique challenges absent or ignored in lab robotics. These range from effects of strong shadows, to variable demonstration quality by non-expert users. With the hope of accelerating research on home robots, and eventually seeing robot butlers in every home, we open-source Dobb-E software stack and models, our data, and our hardware designs at https://dobb-e.com},
	number = {{arXiv}:2311.16098},
	publisher = {{arXiv}},
	author = {Shafiullah, Nur Muhammad Mahi and Rai, Anant and Etukuru, Haritheja and Liu, Yiqian and Misra, Ishan and Chintala, Soumith and Pinto, Lerrel},
	urldate = {2025-03-31},
	date = {2023-11-27},
	eprinttype = {arxiv},
	eprint = {2311.16098 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\FT9SA9CC\\Shafiullah 等 - 2023 - On Bringing Robots Home.pdf:application/pdf;Snapshot:C\:\\Users\\ROG\\Desktop\\大二作业\\lerobot\\参考文献\\storage\\59BJAHQT\\2311.html:text/html},
}
